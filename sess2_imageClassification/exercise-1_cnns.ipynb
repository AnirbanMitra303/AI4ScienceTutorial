{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Digit Classification with CNNs and DNNs\n",
    "The [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) is a database of hand-written digits matched with their actual value that has been exceedingly well used by the machine learning community. It is large and easily describable, which makes it a great example for learning to use convolutional neural networks.\n",
    "\n",
    "This exercise draws extensively from [Keras tutorials](https://github.com/keras-team/keras/blob/master/examples).\n",
    "\n",
    "This notebook contains many sections that are filled out for you and many that you will need to fill out to complete the exercise (marked in <font color='red'>RED</font>). You are finished when \"Restarting and Run All Cells\" executes the entire notebook without producing any errors. Do not remove assert statements.\n",
    "\n",
    "If you get stuck, the correct code is in the cell below. Highlight the text to see it. Example: <font color='white'>print('Hello!')</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from keras import optimizers as opt\n",
    "from keras.datasets import mnist\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from time import perf_counter\n",
    "from math import isclose\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import keras\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some maintenance things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The MNIST data is easily accessible from Keras and needs a little bit of preprocessing before it is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (60000, 28, 28)\n",
      "Output data shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(f'Input data shape: {x_train.shape}')\n",
    "print(f'Output data shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is returned as 60000 28x28 images.\n",
    "Depending on the backend for Keras, we need to turn these into either 28x28x1 or 1x28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data shape: (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'New data shape: {input_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are also integer values between 0 and 255. We need them as single-precision floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are integers between 0 and 9. \n",
    "As we are treating digits as simple categories and not considering the ordering between them, we need to one-hot encode the data.\n",
    "(Recall doing this in the last exercise for the categorical input variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for entry 0: 5\n"
     ]
    }
   ],
   "source": [
    "print(f'Output for entry 0: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New output for entry 0: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f'New output for entry 0: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New output data shape: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f'New output data shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we are now ready to go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Tutorial: Classification Models and Keras\n",
    "All of our previous examples have used regression models.\n",
    "So, some brief lessons on classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Classification Models\n",
    "There are [many ways to rate the quality of classification](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics), each with their own benefits.\n",
    "\n",
    "For example, the [False Positive Rate (FPR)](https://en.wikipedia.org/wiki/False_positive_rate) scores how often your model yields an incorrect prediction.\n",
    "FPR is good for metrics where the cost of reacting to an incorrect positive is high, but would be a poor choice when missing a detection is bad (e.g., fast screening for disease).\n",
    "\n",
    "Our digit classification challenge is simple. We just want to get as many digits correct as possible.\n",
    "Getting more \"0\"s correct is just as important as getting any other digit.\n",
    "So, for that reason, we will use accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score([0, 1, 0], [0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is great for humans to understand the quality of a model but has a big issue if it were used as a loss function: discontinuous derivates.\n",
    "\n",
    "Classification models produce probabilities of an entry (e.g., an image) being in a certain category (e.g., a certain digit) and accuracy scores do not use them well.\n",
    "Small changes in predictions for the probabilities of each entry can lead to step changes in the accuracy.\n",
    "Step changes lead to infinite gradients, which causes problems with gradient decent optimization.\n",
    "\n",
    "So, instead, we use \"log-loss\" or \"categorical cross entry.\" \n",
    "Log-loss has smooth derivates for all changes in probability, which is good for neural network optimization.\n",
    "It also has a nice trait that predictions that are not just correct but more confidently-correct are given better (lower) scores.\n",
    "Ther are other classification quality metrics that have these properties, but log-loss is what we will use today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss([0], [[0.6, 0.4]], labels=[0, 1]) > log_loss([0], [[0.7, 0.3]], labels=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Layers and Keras\n",
    "As you will see, special activation functions are needed for performing classification in Keras.\n",
    "\n",
    "- `softmax` is a good choice for multi-class (i.e., more than 2 classes) classification problem. \n",
    "  It takes a vector of real numbers in and returns them in the range [0, 1] with a sum of 1, which looks like a probability distribution.\n",
    "- `sigmoid` is a good choice for binary classification (only 2 classes) as, like `softmax`, it produces a number on [0, 1]. But, it only takes a single number as input, which makes it simpler than `softmax` to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrating `softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([layers.Dense(10, activation='softmax', input_shape=(2,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(np.array([[-1, 2]]))  # Not trained, so the outputs are meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that all numebrs are between 0 and 1: [[0.17934173 0.1313861  0.22672842 0.03196868 0.04540413 0.02876653\n",
      "  0.10873262 0.08403999 0.07492645 0.08870532]]\n"
     ]
    }
   ],
   "source": [
    "print('Note that all numebrs are between 0 and 1:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And they have a sum of 1 (or close to it):  0.99999994\n"
     ]
    }
   ],
   "source": [
    "print('And they have a sum of 1 (or close to it): ', output.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Train a Fully Connected Neural Network\n",
    "We will train a simple, fully connected neural network (FCNN) as a benchmark using the Keras `Sequential` model.\n",
    "\n",
    "The [`Sequential` model](https://keras.io/getting-started/sequential-model-guide/) is designed for simple neural networks where the inputs one layer feed in to the outputs of the next.\n",
    "Our FCNN will start with a [Flatten](https://keras.io/layers/core/#flatten) layer to shape the data from a Nx28x28x1 array to a Nx784 array.\n",
    "Then, we will use 20 fully-connected (dense) layers to form a neural network.\n",
    "The network ends with a `softmax` layer with 10 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(28, 28, 1)))  # Telling Keras what size image to expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now assign the model a loss function (categorical cross-entropy, which is equivalent to the log-loss described above) and select an optimizer to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('rmsprop', 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we fit the model with enough epochs for it to converge and a reasonable batch size. \n",
    "The `EarlyStopping` callbacks checks if the model has not improved for a certain number of epochs and, if so, rolls back to the best network to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/512\n",
      "54000/54000 [==============================] - 5s 95us/step - loss: 0.2062 - acc: 0.9366 - val_loss: 0.0975 - val_acc: 0.9707\n",
      "Epoch 2/512\n",
      "54000/54000 [==============================] - 5s 90us/step - loss: 0.0857 - acc: 0.9740 - val_loss: 0.1016 - val_acc: 0.9720\n",
      "Epoch 3/512\n",
      "54000/54000 [==============================] - 5s 89us/step - loss: 0.0610 - acc: 0.9820 - val_loss: 0.0844 - val_acc: 0.9785\n",
      "Epoch 4/512\n",
      "54000/54000 [==============================] - 5s 88us/step - loss: 0.0470 - acc: 0.9866 - val_loss: 0.0765 - val_acc: 0.9823\n",
      "Epoch 5/512\n",
      "54000/54000 [==============================] - 5s 88us/step - loss: 0.0392 - acc: 0.9894 - val_loss: 0.1110 - val_acc: 0.9783\n",
      "Epoch 6/512\n",
      "54000/54000 [==============================] - 5s 87us/step - loss: 0.0312 - acc: 0.9913 - val_loss: 0.1018 - val_acc: 0.9800\n",
      "Epoch 7/512\n",
      "54000/54000 [==============================] - 5s 86us/step - loss: 0.0273 - acc: 0.9928 - val_loss: 0.0889 - val_acc: 0.9832\n",
      "Epoch 8/512\n",
      "54000/54000 [==============================] - 5s 87us/step - loss: 0.0242 - acc: 0.9937 - val_loss: 0.1027 - val_acc: 0.9818\n",
      "Epoch 9/512\n",
      "54000/54000 [==============================] - 5s 85us/step - loss: 0.0231 - acc: 0.9947 - val_loss: 0.1402 - val_acc: 0.9797\n",
      "Epoch 10/512\n",
      "54000/54000 [==============================] - 4s 81us/step - loss: 0.0204 - acc: 0.9949 - val_loss: 0.1412 - val_acc: 0.9808\n",
      "Epoch 11/512\n",
      "54000/54000 [==============================] - 4s 81us/step - loss: 0.0183 - acc: 0.9955 - val_loss: 0.1075 - val_acc: 0.9842\n",
      "Epoch 12/512\n",
      "54000/54000 [==============================] - 4s 81us/step - loss: 0.0184 - acc: 0.9961 - val_loss: 0.1390 - val_acc: 0.9810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x134380c0f60>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, epochs=512, validation_split=0.1,\n",
    "          callbacks=[keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on hold-out set:  97.85%\n"
     ]
    }
   ],
   "source": [
    "dnn_accuracy = accuracy_score(model.predict_classes(x_test), y_test)\n",
    "print(f'Accuracy on hold-out set: {dnn_accuracy * 100 : .2f}%')\n",
    "assert dnn_accuracy > 0.975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Make a CNN\n",
    "Now, we are going to build a simple Convolutional Neural Network to do the digit classification.\n",
    "\n",
    "But first, a quick overview of Convolution Neural Networks. \n",
    "\n",
    "The \"convolution\" part of CNN references a type of layer were you apply a \"filter\" centered on each pixel to an image.\n",
    "The filter is a function of the value of that pixel and surrounding filters.\n",
    "\n",
    "For now, let's demonstrate with a \"edge detection filter\". Our goal will be to classify the following images based on whether they contain a vertical line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.zeros((3, 3, 3, 1))\n",
    "dataset[0, :, 0] = 1\n",
    "dataset[1, :, 2] = 1\n",
    "dataset[2, 1, :] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 3)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    axs[i].imshow(np.squeeze(d), cmap='Blues')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is certainly possible to build an FCNN that will do this task, but we can make an even simpler model using a CNN.\n",
    "First, to construct an \"edge detection filter\" in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_filter = np.array([[0, 1, 0], [0, 0, 0], [0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter in this case gives its maximum value if the pixels above and below a pixel are dark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Sequential([layers.Conv2D(filters=1, kernel_size=(3, 3), padding='same', use_bias=False, input_shape=(3, 3, 1))])  # Making a convolution layer for 2D images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.set_weights([v_filter[:, :, None, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = conv.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 3)\n",
    "\n",
    "for i, d in enumerate(filtered):\n",
    "    axs[i].imshow(np.squeeze(d), cmap='Blues', vmax=2, vmin=0)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the images with the vertical lines now have a different maximum pixel value than the one with the horizontal line. \n",
    "A \"MaxPool\" layer in Keras performs such an operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Sequential([layers.MaxPool2D(pool_size=(3, 3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled = pool.predict(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 3)\n",
    "\n",
    "for i, d in enumerate(pooled):\n",
    "    axs[i].imshow(np.atleast_2d(np.squeeze(d)), cmap='Blues', vmax=2, vmin=0)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification is now easy: Dark blue (a value of 2) means vertical line.\n",
    "The beauty of CNNs is that they can learn these filters for you automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Your Turn: A CNN for MNIST</font>\n",
    "\n",
    "The simple \"convolution\" plus \"pooling\" example above is indeed simpler than the common types of CNNs seen in practice.\n",
    "We did not have a last layer that performs the actual classification, and used a \"max pool\" that reduced the image down to a single value.\n",
    "Typically, we want multiple layers of convolutions to learn very complex filters and do not want to reduce an image down to a single pixel between each stage.\n",
    "Further, adding a neural network to learn the actual classification at the end is common.\n",
    "\n",
    "Your task is to train network from [Muhammad Rizwan's tutorial](https://engmrk.com/convolutional-neural-network-3/) with ReLU activation functions: \n",
    "\n",
    "<img width=50% src=\"https://engmrk.com/wp-content/uploads/2018/09/Image-Architecture-of-Convolutional-Neural-Network.png\"/>\n",
    "\n",
    "**HINT**: \"Conv NxMxO\" means O filters with shape NxM\n",
    "\n",
    "**HINT**: There is a Flatten layer between the last Pooling layer and first FC (FC means Dense)\n",
    "\n",
    "**HINT**: Don't forget `input_shape`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='white'>\n",
    "model = Sequential([<br/>\n",
    "    layers.Conv2D(32, (5, 5), input_shape=(img_cols, img_rows, 1)),<br/>\n",
    "    layers.MaxPool2D(pool_size=(2, 2)),<br/>\n",
    "    layers.Conv2D(64, (5, 5)),<br/>\n",
    "    layers.MaxPool2D(pool_size=(2, 2)),<br/>\n",
    "    layers.Flatten(),<br/>\n",
    "    layers.Dense(1024, activation='relu'),<br/>\n",
    "    layers.Dense(10, activation='softmax')<br/>\n",
    "])\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.count_params() == 1111946"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Compile and train it</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/512\n",
      "54000/54000 [==============================] - 8s 150us/step - loss: 0.1277 - val_loss: 0.0549\n",
      "Epoch 2/512\n",
      "54000/54000 [==============================] - 7s 131us/step - loss: 0.0456 - val_loss: 0.0415\n",
      "Epoch 3/512\n",
      "54000/54000 [==============================] - 7s 132us/step - loss: 0.0313 - val_loss: 0.0417\n",
      "Epoch 4/512\n",
      "54000/54000 [==============================] - 7s 135us/step - loss: 0.0224 - val_loss: 0.0451\n",
      "Epoch 5/512\n",
      "54000/54000 [==============================] - 7s 136us/step - loss: 0.0178 - val_loss: 0.0577\n",
      "Epoch 6/512\n",
      "54000/54000 [==============================] - 7s 136us/step - loss: 0.0157 - val_loss: 0.0683\n",
      "Epoch 7/512\n",
      "54000/54000 [==============================] - 7s 135us/step - loss: 0.0125 - val_loss: 0.0522\n",
      "Epoch 8/512\n",
      "54000/54000 [==============================] - 7s 131us/step - loss: 0.0120 - val_loss: 0.0650\n",
      "Epoch 9/512\n",
      "54000/54000 [==============================] - 7s 132us/step - loss: 0.0102 - val_loss: 0.0605\n",
      "Epoch 10/512\n",
      "54000/54000 [==============================] - 7s 135us/step - loss: 0.0088 - val_loss: 0.0736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1343b6dbb38>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='white'>\n",
    "model.compile('rmsprop', 'categorical_crossentropy')<br/>\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=512, validation_split=0.1,<br/>\n",
    "          callbacks=[keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True)])\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on hold-out set:  98.70%\n"
     ]
    }
   ],
   "source": [
    "cnn_accuracy = accuracy_score(model.predict_classes(x_test), y_test)\n",
    "print(f'Accuracy on hold-out set: {cnn_accuracy * 100 : .2f}%')\n",
    "assert cnn_accuracy > 0.985"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy should be higher than your fully-connected nueral network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
